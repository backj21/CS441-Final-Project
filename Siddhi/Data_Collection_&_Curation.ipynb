{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## LINK FOR DATASET: https://drive.google.com/drive/folders/1ks-KSDBHB1FYsXtt_T_HclX-7dxBL7Nw?usp=sharing\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- I. Configuration and Column Mapping ---\n",
        "\n",
        "# The permanent and correct mapping based on your raw data sample\n",
        "CANONICAL_COL_MAP = {\n",
        "    'DAY_OF_WEEK': 'DayOfWeek',\n",
        "    'CRS_DEP_TIME': 'CRSDepTime',\n",
        "    'OP_UNIQUE_CARRIER': 'Reporting_Airline',\n",
        "    'DEST_AIRPORT_ID': 'DestAirportID',\n",
        "    'ORIGIN_AIRPORT_ID': 'OriginAirportID',\n",
        "    'DISTANCE': 'Distance',\n",
        "    'DEP_DELAY': 'DEP_DELAY',\n",
        "    'CANCELLED': 'CANCELLED',\n",
        "    'FL_DATE': 'FL_DATE',\n",
        "    'LATE_AIRCRAFT_DELAY': 'LateAircraftDelay' # Crucial for propagation feature\n",
        "}\n",
        "\n",
        "RAW_REQUIRED_COLS = list(CANONICAL_COL_MAP.keys())\n",
        "\n",
        "# DOT IDs for the Five Strategic Hubs: ORD, MDW, MKE, DTW, and MSP\n",
        "AIRPORT_IDS = [11298, 10821, 13244, 11433, 13487]\n",
        "data_path = '/content/drive/MyDrive/CS441/Final Project/Monthly Raw Data'\n",
        "ORIGIN_COL = 'ORIGIN_AIRPORT_ID'\n",
        "\n",
        "# ----------------------------------------------------\n",
        "print(\"--- Starting Data Assembly and Filtering ---\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Concatenate Files (Recursive Search)\n",
        "# Note: Added '**' and 'recursive=True' to find files in subfolders, which was a fix earlier.\n",
        "file_pattern = os.path.join(data_path, '**', '*.csv')\n",
        "all_files = glob.glob(file_pattern, recursive=True)\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"‚ùå FATAL ERROR: No CSV files found in {data_path}. Check the path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"‚úÖ Found {len(all_files):,} files. Starting concatenation...\")\n",
        "\n",
        "try:\n",
        "    # Use usecols to only load the columns we need, saving memory\n",
        "    list_of_dfs = [pd.read_csv(f, usecols=RAW_REQUIRED_COLS, low_memory=False) for f in all_files]\n",
        "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred during reading or concatenation: {e}\")\n",
        "    exit()\n",
        "\n",
        "initial_total_rows = df.shape[0]\n",
        "print(f\"\\nInitial concatenated dataset size: {initial_total_rows:,} rows.\")\n",
        "\n",
        "# Apply Column Mapping and Deduplication\n",
        "df.rename(columns=CANONICAL_COL_MAP, inplace=True)\n",
        "DEDUP_COLS = ['FL_DATE', 'Reporting_Airline', 'OriginAirportID', 'CRSDepTime']\n",
        "df.drop_duplicates(subset=DEDUP_COLS, inplace=True, keep='first')\n",
        "rows_removed_by_dedup = initial_total_rows - df.shape[0]\n",
        "print(f\"Removed {rows_removed_by_dedup:,} duplicate rows.\")\n",
        "\n",
        "\n",
        "# 2. Filter to Five Hubs and Clean Core Data\n",
        "df['OriginAirportID'] = pd.to_numeric(df['OriginAirportID'], errors='coerce').fillna(0).astype('Int64')\n",
        "df_final = df[df['OriginAirportID'].isin(AIRPORT_IDS)].copy()\n",
        "\n",
        "core_cols_for_check = ['DEP_DELAY', 'CANCELLED', 'CRSDepTime', 'Reporting_Airline']\n",
        "initial_rows = df_final.shape[0]\n",
        "df_final.dropna(subset=core_cols_for_check, inplace=True)\n",
        "df = df_final # Use 'df' for the final working DataFrame\n",
        "\n",
        "final_rows = df.shape[0]\n",
        "print(f\"Filtered and cleaned dataset size: {final_rows:,} rows.\")\n",
        "print(\"--- Data Assembly Complete. Starting Feature Engineering ---\")\n",
        "\n",
        "# --- II. Extraordinary Feature Engineering ---\n",
        "\n",
        "# Feature 1: The Target Variable (Y)\n",
        "print(\"\\n--- Feature 1: Target Variable ---\")\n",
        "df['TARGET_CLASS'] = 0\n",
        "df.loc[(df['DEP_DELAY'] > 15) & (df['CANCELLED'] == 0), 'TARGET_CLASS'] = 1 # Significant Delay\n",
        "df.loc[df['CANCELLED'] == 1, 'TARGET_CLASS'] = 2 # Cancellation\n",
        "print(f\"Target Class Distribution:\\n{df['TARGET_CLASS'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%'}\")\n",
        "df.drop(columns=['DEP_DELAY', 'CANCELLED'], inplace=True)\n",
        "\n",
        "\n",
        "# Feature 2: Cyclical Encoding for Time\n",
        "print(\"\\n--- Feature 2: Cyclical Time Encoding ---\")\n",
        "df['Time_of_Day_Minutes'] = df['CRSDepTime'] // 100 * 60 + df['CRSDepTime'] % 100\n",
        "MAX_MINUTES = 24 * 60\n",
        "\n",
        "# Sin/Cos transformation\n",
        "df['DepTime_sin'] = np.sin(2 * np.pi * df['Time_of_Day_Minutes'] / MAX_MINUTES)\n",
        "df['DepTime_cos'] = np.cos(2 * np.pi * df['Time_of_Day_Minutes'] / MAX_MINUTES)\n",
        "df.drop(columns=['CRSDepTime', 'Time_of_Day_Minutes'], inplace=True)\n",
        "print(\"Created DepTime_sin and DepTime_cos features.\")\n",
        "\n",
        "\n",
        "# Feature 3: The Lagged Delay Propagation Feature\n",
        "print(\"\\n--- Feature 3: Lagged Delay Propagation (Extraordinary Feature) ---\")\n",
        "\n",
        "def calculate_lagged_mean(group, column, window_size=50):\n",
        "    \"\"\"Calculates the rolling mean for a column, shifted by 1.\"\"\"\n",
        "    return group[column].shift(1).rolling(window=window_size, min_periods=1).mean()\n",
        "\n",
        "# Prepare data: Ensure correct data types and chronological sort for the rolling calculation\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
        "# Sort by Hub, then Airline, then Chronologically (Date and Time proxy)\n",
        "df.sort_values(by=['OriginAirportID', 'Reporting_Airline', 'FL_DATE', 'DepTime_sin'], inplace=True)\n",
        "\n",
        "# Lagged_Late_Aircraft: Average LateAircraftDelay for the *previous 50 flights* by this airline at this hub.\n",
        "df['Lagged_Late_Aircraft'] = df.groupby(['OriginAirportID', 'Reporting_Airline']) \\\n",
        "                                 .apply(calculate_lagged_mean, 'LateAircraftDelay', 50) \\\n",
        "                                 .reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# Lagged_Delay_Mean: Average TARGET_CLASS for the *previous 50 flights* by this airline at this hub.\n",
        "df['Lagged_Delay_Mean'] = df.groupby(['OriginAirportID', 'Reporting_Airline']) \\\n",
        "                                 .apply(calculate_lagged_mean, 'TARGET_CLASS', 50) \\\n",
        "                                 .reset_index(level=[0,1], drop=True)\n",
        "\n",
        "df['Lagged_Late_Aircraft'].fillna(0, inplace=True)\n",
        "df['Lagged_Delay_Mean'].fillna(0, inplace=True)\n",
        "df.drop(columns=['LateAircraftDelay'], inplace=True)\n",
        "print(\"Created Lagged_Late_Aircraft and Lagged_Delay_Mean.\")\n",
        "\n",
        "\n",
        "# 4. Final Save (Feature Engineered Data)\n",
        "MASTER_FE_FILE_PATH = '/content/drive/MyDrive/CS441/Final Project/Five_Hub_FE_Master_Data.csv'\n",
        "df.to_csv(MASTER_FE_FILE_PATH, index=False)\n",
        "print(f\"\\nüíæ FINAL FEATURE-ENGINEERED dataset saved to: {MASTER_FE_FILE_PATH}\")\n",
        "print(\"\\n--- NEXT STEP: Categorical Encoding and XGBoost Model Training ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OyUwnhhM_S7U",
        "outputId": "45836a2c-e900-4c55-ba8d-e9a9b9d2d145"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Assembly and Filtering ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Found 46 files. Starting concatenation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1118943904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Use usecols to only load the columns we need, saving memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mlist_of_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAW_REQUIRED_COLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the category_encoders library\n",
        "!pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMFb20ISAxUN",
        "outputId": "7293e196-ebe4-4b9e-d44a-5a095033ec79"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from category_encoders import TargetEncoder\n",
        "import os\n",
        "\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if not os.path.exists(MASTER_FE_FILE_PATH):\n",
        "    print(f\"‚ùå FATAL ERROR: Feature Engineered file not found at {MASTER_FE_FILE_PATH}.\")\n",
        "    print(\"Please confirm the file path in your Google Drive and try again.\")\n",
        "    # Exit or stop execution here if the file is missing\n",
        "    # return\n",
        "else:\n",
        "    print(f\"‚úÖ Loading feature-engineered data from: {MASTER_FE_FILE_PATH}\")\n",
        "    df = pd.read_csv(MASTER_FE_FILE_PATH)\n",
        "\n",
        "    print(\"--- Starting Categorical Encoding and Data Split ---\")\n",
        "\n",
        "    # --- 1. Define Features and Target ---\n",
        "    TARGET = 'TARGET_CLASS'\n",
        "    # Drop target and the date column (FL_DATE) as it's not a direct feature\n",
        "    FEATURES = df.drop(columns=[TARGET, 'FL_DATE']).columns.tolist()\n",
        "\n",
        "    # Identify Categorical Columns for Encoding\n",
        "    CAT_COLS = ['OriginAirportID', 'DestAirportID', 'Reporting_Airline']\n",
        "\n",
        "    # Ensure categorical columns are treated as strings for the encoder\n",
        "    for col in CAT_COLS:\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "    # --- 2. Data Split (Crucial for Target Encoding) ---\n",
        "    X = df[FEATURES]\n",
        "    y = df[TARGET]\n",
        "\n",
        "    # Split data into training and testing sets (80/20 split)\n",
        "    # Stratify ensures the rare classes (1 and 2) are distributed evenly in both sets.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Data split: Training set size: {X_train.shape[0]:,}, Testing set size: {X_test.shape[0]:,}\")\n",
        "\n",
        "    # --- 3. Target Encoding (Applied only to training data) ---\n",
        "    # Target Encoding is necessary for high-cardinality features.\n",
        "    encoder = TargetEncoder(cols=CAT_COLS)\n",
        "\n",
        "    # Fit the encoder ONLY on the training data (y_train) to prevent data leakage.\n",
        "    encoder.fit(X_train, y_train)\n",
        "\n",
        "    # Transform both the training and testing sets.\n",
        "    X_train_encoded = encoder.transform(X_train)\n",
        "    X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "    print(\"Applied Target Encoding to Origin, Destination, and Airline features.\")\n",
        "\n",
        "    # --- 4. Final Data Preparation ---\n",
        "    y_train_int = y_train.astype(int)\n",
        "    y_test_int = y_test.astype(int)\n",
        "\n",
        "    # Save the encoded dataframes for the next step (XGBoost)\n",
        "    # The variables X_train_encoded, X_test_encoded, y_train_int, y_test_int are now ready.\n",
        "    print(\"--- Categorical Encoding and Data Split Complete. Ready for Modeling! ---\")\n",
        "    print(f\"Features ready for XGBoost: {X_train_encoded.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toqC-5kfAkMp",
        "outputId": "0709b2e7-bea5-466c-8fa4-4e3d2e0be66a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loading feature-engineered data from: /content/drive/MyDrive/CS441/Final Project/Five_Hub_FE_Master_Data.csv\n",
            "--- Starting Categorical Encoding and Data Split ---\n",
            "Data split: Training set size: 89,419, Testing set size: 22,355\n",
            "Applied Target Encoding to Origin, Destination, and Airline features.\n",
            "--- Categorical Encoding and Data Split Complete. Ready for Modeling! ---\n",
            "Features ready for XGBoost: ['DayOfWeek', 'Reporting_Airline', 'OriginAirportID', 'DestAirportID', 'Distance', 'DepTime_sin', 'DepTime_cos', 'Lagged_Late_Aircraft', 'Lagged_Delay_Mean']\n"
          ]
        }
      ]
    }
  ]
}