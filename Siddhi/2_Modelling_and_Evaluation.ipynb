{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## LINK FOR DATASET: https://drive.google.com/drive/folders/1ks-KSDBHB1FYsXtt_T_HclX-7dxBL7Nw?usp=sharing\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- I. Configuration and Column Mapping ---\n",
        "\n",
        "# The permanent and correct mapping based on your raw data sample\n",
        "CANONICAL_COL_MAP = {\n",
        "    'DAY_OF_WEEK': 'DayOfWeek',\n",
        "    'CRS_DEP_TIME': 'CRSDepTime',\n",
        "    'OP_UNIQUE_CARRIER': 'Reporting_Airline',\n",
        "    'DEST_AIRPORT_ID': 'DestAirportID',\n",
        "    'ORIGIN_AIRPORT_ID': 'OriginAirportID',\n",
        "    'DISTANCE': 'Distance',\n",
        "    'DEP_DELAY': 'DEP_DELAY',\n",
        "    'CANCELLED': 'CANCELLED',\n",
        "    'FL_DATE': 'FL_DATE',\n",
        "    'LATE_AIRCRAFT_DELAY': 'LateAircraftDelay' # Crucial for propagation feature\n",
        "}\n",
        "\n",
        "RAW_REQUIRED_COLS = list(CANONICAL_COL_MAP.keys())\n",
        "\n",
        "# DOT IDs for the Five Strategic Hubs: ORD, MDW, MKE, DTW, and MSP\n",
        "AIRPORT_IDS = [11298, 10821, 13244, 11433, 13487]\n",
        "data_path = '/content/drive/MyDrive/CS441/Final Project/Monthly Raw Data'\n",
        "ORIGIN_COL = 'ORIGIN_AIRPORT_ID'\n",
        "\n",
        "# ----------------------------------------------------\n",
        "print(\"--- Starting Data Assembly and Filtering ---\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Concatenate Files (Recursive Search)\n",
        "# Note: Added '**' and 'recursive=True' to find files in subfolders, which was a fix earlier.\n",
        "file_pattern = os.path.join(data_path, '**', '*.csv')\n",
        "all_files = glob.glob(file_pattern, recursive=True)\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"‚ùå FATAL ERROR: No CSV files found in {data_path}. Check the path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"‚úÖ Found {len(all_files):,} files. Starting concatenation...\")\n",
        "\n",
        "try:\n",
        "    # Use usecols to only load the columns we need, saving memory\n",
        "    list_of_dfs = [pd.read_csv(f, usecols=RAW_REQUIRED_COLS, low_memory=False) for f in all_files]\n",
        "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred during reading or concatenation: {e}\")\n",
        "    exit()\n",
        "\n",
        "initial_total_rows = df.shape[0]\n",
        "print(f\"\\nInitial concatenated dataset size: {initial_total_rows:,} rows.\")\n",
        "\n",
        "# Apply Column Mapping and Deduplication\n",
        "df.rename(columns=CANONICAL_COL_MAP, inplace=True)\n",
        "DEDUP_COLS = ['FL_DATE', 'Reporting_Airline', 'OriginAirportID', 'CRSDepTime']\n",
        "df.drop_duplicates(subset=DEDUP_COLS, inplace=True, keep='first')\n",
        "rows_removed_by_dedup = initial_total_rows - df.shape[0]\n",
        "print(f\"Removed {rows_removed_by_dedup:,} duplicate rows.\")\n",
        "\n",
        "\n",
        "# 2. Filter to Five Hubs and Clean Core Data\n",
        "df['OriginAirportID'] = pd.to_numeric(df['OriginAirportID'], errors='coerce').fillna(0).astype('Int64')\n",
        "df_final = df[df['OriginAirportID'].isin(AIRPORT_IDS)].copy()\n",
        "\n",
        "core_cols_for_check = ['DEP_DELAY', 'CANCELLED', 'CRSDepTime', 'Reporting_Airline']\n",
        "initial_rows = df_final.shape[0]\n",
        "df_final.dropna(subset=core_cols_for_check, inplace=True)\n",
        "df = df_final # Use 'df' for the final working DataFrame\n",
        "\n",
        "final_rows = df.shape[0]\n",
        "print(f\"Filtered and cleaned dataset size: {final_rows:,} rows.\")\n",
        "print(\"--- Data Assembly Complete. Starting Feature Engineering ---\")\n",
        "\n",
        "# --- II. Extraordinary Feature Engineering ---\n",
        "\n",
        "# Feature 1: The Target Variable (Y)\n",
        "print(\"\\n--- Feature 1: Target Variable ---\")\n",
        "df['TARGET_CLASS'] = 0\n",
        "df.loc[(df['DEP_DELAY'] > 15) & (df['CANCELLED'] == 0), 'TARGET_CLASS'] = 1 # Significant Delay\n",
        "df.loc[df['CANCELLED'] == 1, 'TARGET_CLASS'] = 2 # Cancellation\n",
        "print(f\"Target Class Distribution:\\n{df['TARGET_CLASS'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%'}\")\n",
        "df.drop(columns=['DEP_DELAY', 'CANCELLED'], inplace=True)\n",
        "\n",
        "\n",
        "# Feature 2: Cyclical Encoding for Time\n",
        "print(\"\\n--- Feature 2: Cyclical Time Encoding ---\")\n",
        "df['Time_of_Day_Minutes'] = df['CRSDepTime'] // 100 * 60 + df['CRSDepTime'] % 100\n",
        "MAX_MINUTES = 24 * 60\n",
        "\n",
        "# Sin/Cos transformation\n",
        "df['DepTime_sin'] = np.sin(2 * np.pi * df['Time_of_Day_Minutes'] / MAX_MINUTES)\n",
        "df['DepTime_cos'] = np.cos(2 * np.pi * df['Time_of_Day_Minutes'] / MAX_MINUTES)\n",
        "df.drop(columns=['CRSDepTime', 'Time_of_Day_Minutes'], inplace=True)\n",
        "print(\"Created DepTime_sin and DepTime_cos features.\")\n",
        "\n",
        "\n",
        "# Feature 3: The Lagged Delay Propagation Feature\n",
        "print(\"\\n--- Feature 3: Lagged Delay Propagation (Extraordinary Feature) ---\")\n",
        "\n",
        "def calculate_lagged_mean(group, column, window_size=50):\n",
        "    \"\"\"Calculates the rolling mean for a column, shifted by 1.\"\"\"\n",
        "    return group[column].shift(1).rolling(window=window_size, min_periods=1).mean()\n",
        "\n",
        "# Prepare data: Ensure correct data types and chronological sort for the rolling calculation\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
        "# Sort by Hub, then Airline, then Chronologically (Date and Time proxy)\n",
        "df.sort_values(by=['OriginAirportID', 'Reporting_Airline', 'FL_DATE', 'DepTime_sin'], inplace=True)\n",
        "\n",
        "# Lagged_Late_Aircraft: Average LateAircraftDelay for the *previous 50 flights* by this airline at this hub.\n",
        "df['Lagged_Late_Aircraft'] = df.groupby(['OriginAirportID', 'Reporting_Airline']) \\\n",
        "                                 .apply(calculate_lagged_mean, 'LateAircraftDelay', 50) \\\n",
        "                                 .reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# Lagged_Delay_Mean: Average TARGET_CLASS for the *previous 50 flights* by this airline at this hub.\n",
        "df['Lagged_Delay_Mean'] = df.groupby(['OriginAirportID', 'Reporting_Airline']) \\\n",
        "                                 .apply(calculate_lagged_mean, 'TARGET_CLASS', 50) \\\n",
        "                                 .reset_index(level=[0,1], drop=True)\n",
        "\n",
        "df['Lagged_Late_Aircraft'].fillna(0, inplace=True)\n",
        "df['Lagged_Delay_Mean'].fillna(0, inplace=True)\n",
        "df.drop(columns=['LateAircraftDelay'], inplace=True)\n",
        "print(\"Created Lagged_Late_Aircraft and Lagged_Delay_Mean.\")\n",
        "\n",
        "\n",
        "# 4. Final Save (Feature Engineered Data)\n",
        "MASTER_FE_FILE_PATH = '/content/drive/MyDrive/CS441/Final Project/Five_Hub_FE_Master_Data.csv'\n",
        "df.to_csv(MASTER_FE_FILE_PATH, index=False)\n",
        "print(f\"\\nüíæ FINAL FEATURE-ENGINEERED dataset saved to: {MASTER_FE_FILE_PATH}\")\n",
        "print(\"\\n--- NEXT STEP: Categorical Encoding and XGBoost Model Training ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyUwnhhM_S7U",
        "outputId": "630251bf-b2d7-4acb-f4f1-9dd855b3a6a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Assembly and Filtering ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Found 46 files. Starting concatenation...\n",
            "\n",
            "Initial concatenated dataset size: 2,771,870 rows.\n",
            "Removed 591,317 duplicate rows.\n",
            "Filtered and cleaned dataset size: 111,774 rows.\n",
            "--- Data Assembly Complete. Starting Feature Engineering ---\n",
            "\n",
            "--- Feature 1: Target Variable ---\n",
            "Target Class Distribution:\n",
            "TARGET_CLASS\n",
            "0    80.26%\n",
            "1     19.7%\n",
            "2     0.04%\n",
            "Name: proportion, dtype: object\n",
            "\n",
            "--- Feature 2: Cyclical Time Encoding ---\n",
            "Created DepTime_sin and DepTime_cos features.\n",
            "\n",
            "--- Feature 3: Lagged Delay Propagation (Extraordinary Feature) ---\n",
            "Created Lagged_Late_Aircraft and Lagged_Delay_Mean.\n",
            "\n",
            "üíæ FINAL FEATURE-ENGINEERED dataset saved to: /content/drive/MyDrive/CS441/Final Project/Five_Hub_FE_Master_Data.csv\n",
            "\n",
            "--- NEXT STEP: Categorical Encoding and XGBoost Model Training ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the category_encoders library\n",
        "!pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMFb20ISAxUN",
        "outputId": "b50c95a6-521e-4b92-e77e-bc1e66159071"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from category_encoders import TargetEncoder\n",
        "import os\n",
        "\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if not os.path.exists(MASTER_FE_FILE_PATH):\n",
        "    print(f\"FATAL ERROR: Feature Engineered file not found at {MASTER_FE_FILE_PATH}.\")\n",
        "    print(\"Please confirm the file path in your Google Drive and try again.\")\n",
        "    # Exit or stop execution here if the file is missing\n",
        "    # return\n",
        "else:\n",
        "    print(f\"Loading feature-engineered data from: {MASTER_FE_FILE_PATH}\")\n",
        "    df = pd.read_csv(MASTER_FE_FILE_PATH)\n",
        "\n",
        "    print(\"--- Starting Categorical Encoding and Data Split ---\")\n",
        "\n",
        "    # --- 1. Define Features and Target ---\n",
        "    TARGET = 'TARGET_CLASS'\n",
        "    # Drop target and the date column (FL_DATE) as it's not a direct feature\n",
        "    FEATURES = df.drop(columns=[TARGET, 'FL_DATE']).columns.tolist()\n",
        "\n",
        "    # Identify Categorical Columns for Encoding\n",
        "    CAT_COLS = ['OriginAirportID', 'DestAirportID', 'Reporting_Airline']\n",
        "\n",
        "    # Ensure categorical columns are treated as strings for the encoder\n",
        "    for col in CAT_COLS:\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "    # --- 2. Data Split (Crucial for Target Encoding) ---\n",
        "    X = df[FEATURES]\n",
        "    y = df[TARGET]\n",
        "\n",
        "    # Split data into training and testing sets (80/20 split)\n",
        "    # Stratify ensures the rare classes (1 and 2) are distributed evenly in both sets.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Data split: Training set size: {X_train.shape[0]:,}, Testing set size: {X_test.shape[0]:,}\")\n",
        "\n",
        "    # --- 3. Target Encoding (Applied only to training data) ---\n",
        "    # Target Encoding is necessary for high-cardinality features.\n",
        "    encoder = TargetEncoder(cols=CAT_COLS)\n",
        "\n",
        "    # Fit the encoder ONLY on the training data (y_train) to prevent data leakage.\n",
        "    encoder.fit(X_train, y_train)\n",
        "\n",
        "    # Transform both the training and testing sets.\n",
        "    X_train_encoded = encoder.transform(X_train)\n",
        "    X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "    print(\"Applied Target Encoding to Origin, Destination, and Airline features.\")\n",
        "\n",
        "    # --- 4. Final Data Preparation ---\n",
        "    y_train_int = y_train.astype(int)\n",
        "    y_test_int = y_test.astype(int)\n",
        "\n",
        "    # Save the encoded dataframes for the next step (XGBoost)\n",
        "    # The variables X_train_encoded, X_test_encoded, y_train_int, y_test_int are now ready.\n",
        "    print(\"--- Categorical Encoding and Data Split Complete. Ready for Modeling! ---\")\n",
        "    print(f\"Features ready for XGBoost: {X_train_encoded.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toqC-5kfAkMp",
        "outputId": "a2c1f4f8-7453-4ca8-a600-24704ecfeacc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading feature-engineered data from: /content/drive/MyDrive/CS441/Final Project/Five_Hub_FE_Master_Data.csv\n",
            "--- Starting Categorical Encoding and Data Split ---\n",
            "Data split: Training set size: 89,419, Testing set size: 22,355\n",
            "Applied Target Encoding to Origin, Destination, and Airline features.\n",
            "--- Categorical Encoding and Data Split Complete. Ready for Modeling! ---\n",
            "Features ready for XGBoost: ['DayOfWeek', 'Reporting_Airline', 'OriginAirportID', 'DestAirportID', 'Distance', 'DepTime_sin', 'DepTime_cos', 'Lagged_Late_Aircraft', 'Lagged_Delay_Mean']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"--- Starting 5-Model Hyperparameter Search (Targeting Extraordinary Score) ---\")\n",
        "\n",
        "# --- DATA SIZE SUMMARY (FOR REPORT) ---\n",
        "# NOTE: The variables X_train_encoded and X_test_encoded are available from the previous step.\n",
        "# We explicitly report the size here as requested.\n",
        "print(f\"\\n‚úÖ Data Split Summary for Report:\")\n",
        "print(f\"   Training Set Size (X_train): {X_train_encoded.shape[0]:,} rows\")\n",
        "print(f\"   Testing Set Size (X_test):   {X_test_encoded.shape[0]:,} rows\")\n",
        "print(f\"   Total Samples Used:          {X_train_encoded.shape[0] + X_test_encoded.shape[0]:,} rows\")\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "# Define five distinct parameter configurations for a comprehensive search\n",
        "param_sets = {\n",
        "    \"Low_Complexity\": {     # Baseline, faster training\n",
        "        'n_estimators': 100,\n",
        "        'max_depth': 4,\n",
        "        'learning_rate': 0.15\n",
        "    },\n",
        "    \"Optimal_Balance\": {    # Strong Standard Configuration\n",
        "        'n_estimators': 150,\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.1\n",
        "    },\n",
        "    \"High_Complexity\": {    # Aggressive training, high risk of overfitting\n",
        "        'n_estimators': 250,\n",
        "        'max_depth': 8,\n",
        "        'learning_rate': 0.05\n",
        "    },\n",
        "    \"Low_Learning_Rate\": {  # Slow convergence, high potential for detailed feature discovery\n",
        "        'n_estimators': 200,\n",
        "        'max_depth': 7,\n",
        "        'learning_rate': 0.02\n",
        "    },\n",
        "    \"Aggressive_Learning\": {# Fast convergence, potential for faster result, but less precise\n",
        "        'n_estimators': 120,\n",
        "        'max_depth': 5,\n",
        "        'learning_rate': 0.25\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate Class Weights\n",
        "class_counts = y_train_int.value_counts().sort_index()\n",
        "total_samples = len(y_train_int)\n",
        "scale_factor = total_samples / (len(class_counts) * class_counts)\n",
        "sample_weights = y_train_int.map(scale_factor).values\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Iterate through all parameter sets\n",
        "for name, params in param_sets.items():\n",
        "    print(f\"\\nTraining Model: {name}...\")\n",
        "\n",
        "    # CRITICAL FIX: eval_metric is passed during initialization for this XGBoost version\n",
        "    xgb_model = XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=3,\n",
        "        random_state=42,\n",
        "        tree_method='hist',\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric='mlogloss',\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(\n",
        "        X_train_encoded,\n",
        "        y_train_int,\n",
        "        sample_weight=sample_weights,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    y_pred = xgb_model.predict(X_test_encoded)\n",
        "\n",
        "    # Use Weighted F1-Score as the primary evaluation metric\n",
        "    f1_weighted = f1_score(y_test_int, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    results[name] = {\n",
        "        'F1_Score': f1_weighted,\n",
        "        'Model': xgb_model,\n",
        "        'Predictions': y_pred\n",
        "    }\n",
        "\n",
        "    print(f\"  {name} Weighted F1-Score: {f1_weighted:.4f}\")\n",
        "\n",
        "# --- 2. Select the Best Model ---\n",
        "best_model_name = max(results, key=lambda k: results[k]['F1_Score'])\n",
        "best_result = results[best_model_name]\n",
        "best_f1 = best_result['F1_Score']\n",
        "best_predictions = best_result['Predictions']\n",
        "y_test = y_test_int\n",
        "\n",
        "print(f\"\\n--- Model Selection Complete ---\")\n",
        "print(f\"üèÜ Best Model Selected: {best_model_name} (F1-Score: {best_f1:.4f})\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "\n",
        "# --- 3. Final Evaluation of the Best Model ---\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\n--- Detailed Classification Report (Labels: 0=OnTime, 1=Delay, 2=Cancel) ---\")\n",
        "print(classification_report(y_test, best_predictions, target_names=['OnTime', 'Delay', 'Cancel'], zero_division=0))\n",
        "\n",
        "# Confusion Matrix (Crucial for visualization and analysis)\n",
        "conf_mat = confusion_matrix(y_test, best_predictions)\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(conf_mat)\n",
        "\n",
        "\n",
        "# Prepare the Hyperparameter Comparison Table for the report\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(param_sets.keys()),\n",
        "    'n_estimators': [p['n_estimators'] for p in param_sets.values()],\n",
        "    'max_depth': [p['max_depth'] for p in param_sets.values()],\n",
        "    'learning_rate': [p['learning_rate'] for p in param_sets.values()],\n",
        "    'Weighted F1-Score': [results[name]['F1_Score'] for name in param_sets.keys()]\n",
        "}).round(4)\n",
        "\n",
        "print(\"\\n--- Hyperparameter Comparison Table for Report ---\")\n",
        "print(comparison_df.to_markdown(index=False))\n",
        "\n",
        "print(\"\\n--- Analysis Complete. Ready for Final Report Section Drafting ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVs7h5O84hX4",
        "outputId": "66b9c123-f0fe-47de-c793-5ac51ca2bc74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting 5-Model Hyperparameter Search (Targeting Extraordinary Score) ---\n",
            "\n",
            "‚úÖ Data Split Summary for Report:\n",
            "   Training Set Size (X_train): 89,419 rows\n",
            "   Testing Set Size (X_test):   22,355 rows\n",
            "   Total Samples Used:          111,774 rows\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Training Model: Low_Complexity...\n",
            "  Low_Complexity Weighted F1-Score: 0.6779\n",
            "\n",
            "Training Model: Optimal_Balance...\n",
            "  Optimal_Balance Weighted F1-Score: 0.6890\n",
            "\n",
            "Training Model: High_Complexity...\n",
            "  High_Complexity Weighted F1-Score: 0.7013\n",
            "\n",
            "Training Model: Low_Learning_Rate...\n",
            "  Low_Learning_Rate Weighted F1-Score: 0.6918\n",
            "\n",
            "Training Model: Aggressive_Learning...\n",
            "  Aggressive_Learning Weighted F1-Score: 0.6841\n",
            "\n",
            "--- Model Selection Complete ---\n",
            "üèÜ Best Model Selected: High_Complexity (F1-Score: 0.7013)\n",
            "---------------------------------\n",
            "\n",
            "--- Detailed Classification Report (Labels: 0=OnTime, 1=Delay, 2=Cancel) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      OnTime       0.87      0.70      0.77     17941\n",
            "       Delay       0.32      0.57      0.41      4405\n",
            "      Cancel       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.67     22355\n",
            "   macro avg       0.40      0.42      0.39     22355\n",
            "weighted avg       0.76      0.67      0.70     22355\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[12505  5412    24]\n",
            " [ 1877  2519     9]\n",
            " [    4     5     0]]\n",
            "\n",
            "--- Hyperparameter Comparison Table for Report ---\n",
            "| Model               |   n_estimators |   max_depth |   learning_rate |   Weighted F1-Score |\n",
            "|:--------------------|---------------:|------------:|----------------:|--------------------:|\n",
            "| Low_Complexity      |            100 |           4 |            0.15 |              0.6779 |\n",
            "| Optimal_Balance     |            150 |           6 |            0.1  |              0.689  |\n",
            "| High_Complexity     |            250 |           8 |            0.05 |              0.7013 |\n",
            "| Low_Learning_Rate   |            200 |           7 |            0.02 |              0.6918 |\n",
            "| Aggressive_Learning |            120 |           5 |            0.25 |              0.6841 |\n",
            "\n",
            "--- Analysis Complete. Ready for Final Report Section Drafting ---\n"
          ]
        }
      ]
    }
  ]
}